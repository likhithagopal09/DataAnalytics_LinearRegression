# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WXuq-1w9P_HBTjdrS7_zqWlFmhOELPh2
"""

# Step 0 â€” Imports
import os, io, zipfile, glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.inspection import permutation_importance

# Step 2
from google.colab import files

uploaded = files.upload()  # <-- This opens the file chooser

# Save uploaded files and auto-unzip if needed
csv_candidates = []

for name, data in uploaded.items():
    path = f"/content/{name}"
    with open(path, "wb") as f:
        f.write(data)

    if name.lower().endswith(".zip"):
        print(f"Unzipping: {name}")
        with zipfile.ZipFile(io.BytesIO(data)) as z:
            z.extractall("/content")
    elif name.lower().endswith(".csv"):
        csv_candidates.append(path)

if not csv_candidates:
    csv_candidates = glob.glob("/content/**/*.csv", recursive=True) + glob.glob("/content/*.csv")

if not csv_candidates:
    raise FileNotFoundError("No CSV found. Please upload a CSV or a ZIP containing CSV(s).")

csv_path = None
for p in csv_candidates:
    if os.path.basename(p).lower() == "housing.csv":
        csv_path = p
        break
if csv_path is None:
    csv_path = csv_candidates[0]

print(f"Selected CSV: {csv_path}")

# Step 3 â€” Load and peek
df = pd.read_csv(csv_path)
print("Shape:", df.shape)
display(df.head())
print("\nColumns:", df.columns.tolist())
print("\nInfo:")
print(df.info())
print("\nMissing values per column:")
print(df.isna().sum())

# Step 4
# Kaggle file typically has: area, bedrooms, price, furnishingstatus
rename_map = {}
cols_lower = {c.lower(): c for c in df.columns}

if "area" in cols_lower:          rename_map[cols_lower["area"]] = "Size"
if "bedrooms" in cols_lower:      rename_map[cols_lower["bedrooms"]] = "Number of Rooms"
if "price" in cols_lower:         rename_map[cols_lower["price"]] = "Price"

if "furnishingstatus" in cols_lower:
    rename_map[cols_lower["furnishingstatus"]] = "Location"

df = df.rename(columns=rename_map)

required = ["Size", "Number of Rooms", "Price"]
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError(f"Required columns missing after mapping: {missing}\nAvailable: {df.columns.tolist()}")

if "Location" not in df.columns:
    # Try some other categorical columns if present
    cat_cols = [c for c in df.columns if df[c].dtype == "object" and c not in ["Price"]]
    if cat_cols:
        df = df.rename(columns={cat_cols[0]: "Location"})
        print(f"'Location' not found; using '{cat_cols[0]}' as 'Location'.")
    else:
        # If truly none, create a single dummy category
        df["Location"] = "Unknown"
        print("No categorical column found; created dummy 'Location'='Unknown'.")

# Drop rows with missing target
df = df.dropna(subset=["Price"]).reset_index(drop=True)
print("Final columns:", df.columns.tolist())
display(df.head())

# Step 5 â€” Distributions (numerical)
for col in ["Size", "Number of Rooms", "Price"]:
    plt.figure()
    plt.hist(df[col].dropna(), bins=30)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col); plt.ylabel("Count")
    plt.show()

# Outlier summary via IQR
def outlier_summary(s):
    q1, q3 = s.quantile(0.25), s.quantile(0.75)
    iqr = q3 - q1
    lo, hi = q1 - 1.5*iqr, q3 + 1.5*iqr
    return pd.Series({"Q1": q1, "Q3": q3, "IQR": iqr, "Lower": lo, "Upper": hi,
                      "Outliers": ((s < lo) | (s > hi)).sum()})

summary = pd.concat({c: outlier_summary(df[c]) for c in ["Size", "Number of Rooms", "Price"]}, axis=1).T
print("Outlier summary (IQR):")
display(summary)

#show boxplots
for col in ["Size", "Number of Rooms", "Price"]:
    plt.figure()
    df[[col]].boxplot()
    plt.title(f"Boxplot: {col}")
    plt.show()

# Step 6 â€” Correlation with Price
corr = df[["Size", "Number of Rooms", "Price"]].corr()
print("\nCorrelation matrix:")
display(corr)
print("\nCorrelation with Price:")
print(corr["Price"])

# Step 7 â€” Features & target
X = df[["Size", "Number of Rooms", "Location"]]
y = df["Price"]

numeric_features = ["Size", "Number of Rooms"]
categorical_features = ["Location"]

# Step 8 â€” Preprocessing pipelines
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

# Handle sklearn version differences for OneHotEncoder argument
try:
    ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
except TypeError:
    ohe = OneHotEncoder(handle_unknown="ignore", sparse=False)

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", ohe)
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ],
    remainder="drop"
)

# Step 9 â€” Split (80/20, reproducible)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
len(X_train), len(X_test)

# Step 10 â€” Model training
model = Pipeline(steps=[
    ("preprocess", preprocess),
    ("lr", LinearRegression())
])

model.fit(X_train, y_train)
print("Model trained.")

# Step 11 â€” Predict and inspect
y_pred = model.predict(X_test)
pred_df = pd.DataFrame({"Actual": y_test.values, "Predicted": y_pred})
display(pred_df.head())

# Save predictions
pred_path = "/content/test_predictions.csv"
pred_df.to_csv(pred_path, index=False)
print(f"Saved predictions to: {pred_path}")

# Step 12 â€” Evaluation Metrics
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Mean Squared Error
mse = mean_squared_error(y_test, y_pred)

# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# RÂ² Score
r2 = r2_score(y_test, y_pred)

print("ðŸ“Š Model Evaluation Results")
print(f"RMSE : {rmse:.2f}")
print(f"RÂ²   : {r2:.4f}")

# Step 13 â€” Predicted vs Actual
plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred, alpha=0.6)
mn, mx = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())
plt.plot([mn, mx], [mn, mx])
plt.xlabel("Actual Price"); plt.ylabel("Predicted Price")
plt.title("Predicted vs Actual Price")
plt.show()

# Residuals
residuals = y_test - y_pred
plt.figure()
plt.hist(residuals, bins=30)
plt.title("Residuals Distribution")
plt.xlabel("Residual (Actual - Predicted)")
plt.ylabel("Count")
plt.show()

# Step 14 â€” Feature Insights
import pandas as pd
from sklearn.inspection import permutation_importance

# Get the actual feature names from the transformed dataset
feature_names = X_train.columns

perm = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)

# Create DataFrame safely
perm_df = pd.DataFrame({
    "Feature": feature_names,
    "Importance_Mean": perm.importances_mean,
    "Importance_Std": perm.importances_std
}).sort_values(by="Importance_Mean", ascending=False)

print("Feature Importance:")
print(perm_df)

# bar plot for better visualization
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.barh(perm_df["Feature"], perm_df["Importance_Mean"], xerr=perm_df["Importance_Std"], color="skyblue")
plt.gca().invert_yaxis()
plt.xlabel("Mean Importance")
plt.title("Feature Importance (Permutation)")
plt.show()

# Step 15 â€” Save trained model
import joblib
model_path = "/content/linear_regression_house_prices.joblib"
joblib.dump(model, model_path)
print(f"Model saved to: {model_path}")